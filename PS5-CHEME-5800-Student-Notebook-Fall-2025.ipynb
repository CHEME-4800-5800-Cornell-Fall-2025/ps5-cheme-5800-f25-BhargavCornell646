{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fd3d6b-cedb-4070-84db-9b1d09a0f271",
   "metadata": {},
   "source": [
    "# PS5: The Apples Versus Oranges Problem as a Markov Decision Process\n",
    "Problem set 5 (PS5) tests the hypothesis that a classical optimization problem, such as selecting the optimal consumption bundle of apples and oranges, can be structured as a Markov Decision Process (MDP), where an optimal policy can be computed using value iteration. Toward this objective, there are two problems that we need to solve:\n",
    "\n",
    "* __Problem 1__: In this problem, we solve the apples and oranges problem subject to a budget constraint as a nonlinear programming problem [using the `MadNLP.jl` package](https://github.com/MadNLP/MadNLP.jl). The optimal solution to this problem will be used as the terminal state for the MDP calculations.\n",
    "* __Problem 2__: In this problem, we construct an MDP problem encoding the apples and oranges decision and solve for the optimal policy function $\\pi$ using value iteration. In this problem, we enforce the budget constraint as a soft-wall constraint and use reward shaping to help the search.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5d878-10d4-4af8-85fa-21c55cde92fa",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25550134-a286-4aa1-8aee-19a7bd80d1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl.git`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m JSON ‚îÄ v1.3.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m JuMP ‚îÄ v1.29.3\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646\\Project.toml`\n",
      "  \u001b[90m[5ae59095] \u001b[39m\u001b[92m+ Colors v0.13.1\u001b[39m\n",
      "  \u001b[90m[a93c6f00] \u001b[39m\u001b[92m+ DataFrames v1.8.1\u001b[39m\n",
      "  \u001b[90m[31c24e10] \u001b[39m\u001b[92m+ Distributions v0.25.122\u001b[39m\n",
      "  \u001b[90m[7073ff75] \u001b[39m\u001b[92m+ IJulia v1.32.1\u001b[39m\n",
      "  \u001b[90m[4076af6c] \u001b[39m\u001b[92m+ JuMP v1.29.3\u001b[39m\n",
      "  \u001b[90m[2621e9c9] \u001b[39m\u001b[92m+ MadNLP v0.8.12\u001b[39m\n",
      "  \u001b[90m[91a5bcdd] \u001b[39m\u001b[92m+ Plots v1.41.1\u001b[39m\n",
      "  \u001b[90m[08abe8d2] \u001b[39m\u001b[92m+ PrettyTables v3.1.0\u001b[39m\n",
      "  \u001b[90m[10745b16] \u001b[39m\u001b[92m+ Statistics v1.11.1\u001b[39m\n",
      "  \u001b[90m[24b76065] \u001b[39m\u001b[92m+ VLDataScienceMachineLearningPackage v0.1.0 `https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl.git#main`\u001b[39m\n",
      "  \u001b[90m[37e2e46d] \u001b[39m\u001b[93m~ LinearAlgebra ‚áí v1.12.0\u001b[39m\n",
      "  \u001b[90m[8dfed614] \u001b[39m\u001b[93m~ Test ‚áí v1.11.0\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646\\Manifest.toml`\n",
      "  \u001b[90m[14f7f29c] \u001b[39m\u001b[92m+ AMD v0.5.3\u001b[39m\n",
      "  \u001b[90m[66dad0bd] \u001b[39m\u001b[92m+ AliasTables v1.1.3\u001b[39m\n",
      "  \u001b[90m[6e4b80f9] \u001b[39m\u001b[92m+ BenchmarkTools v1.6.3\u001b[39m\n",
      "  \u001b[90m[d1d4a3ce] \u001b[39m\u001b[92m+ BitFlags v0.1.9\u001b[39m\n",
      "  \u001b[90m[336ed68f] \u001b[39m\u001b[92m+ CSV v0.10.15\u001b[39m\n",
      "  \u001b[90m[523fee87] \u001b[39m\u001b[92m+ CodecBzip2 v0.8.5\u001b[39m\n",
      "  \u001b[90m[944b1d66] \u001b[39m\u001b[92m+ CodecZlib v0.7.8\u001b[39m\n",
      "  \u001b[90m[35d6a980] \u001b[39m\u001b[92m+ ColorSchemes v3.31.0\u001b[39m\n",
      "  \u001b[90m[3da002f7] \u001b[39m\u001b[92m+ ColorTypes v0.12.1\u001b[39m\n",
      "  \u001b[90m[c3611d14] \u001b[39m\u001b[92m+ ColorVectorSpace v0.11.0\u001b[39m\n",
      "  \u001b[90m[5ae59095] \u001b[39m\u001b[92m+ Colors v0.13.1\u001b[39m\n",
      "  \u001b[90m[bbf7d656] \u001b[39m\u001b[92m+ CommonSubexpressions v0.3.1\u001b[39m\n",
      "  \u001b[90m[34da2185] \u001b[39m\u001b[92m+ Compat v4.18.1\u001b[39m\n",
      "  \u001b[90m[f0e56b4a] \u001b[39m\u001b[92m+ ConcurrentUtilities v2.5.0\u001b[39m\n",
      "  \u001b[90m[8f4d0f93] \u001b[39m\u001b[92m+ Conda v1.10.3\u001b[39m\n",
      "  \u001b[90m[d38c429a] \u001b[39m\u001b[92m+ Contour v0.6.3\u001b[39m\n",
      "  \u001b[90m[a8cc5b0e] \u001b[39m\u001b[92m+ Crayons v4.1.1\u001b[39m\n",
      "  \u001b[90m[9a962f9c] \u001b[39m\u001b[92m+ DataAPI v1.16.0\u001b[39m\n",
      "  \u001b[90m[a93c6f00] \u001b[39m\u001b[92m+ DataFrames v1.8.1\u001b[39m\n",
      "  \u001b[90m[864edb3b] \u001b[39m\u001b[92m+ DataStructures v0.19.3\u001b[39m\n",
      "  \u001b[90m[e2d170a0] \u001b[39m\u001b[92m+ DataValueInterfaces v1.0.0\u001b[39m\n",
      "  \u001b[90m[8bb1440f] \u001b[39m\u001b[92m+ DelimitedFiles v1.9.1\u001b[39m\n",
      "  \u001b[90m[163ba53b] \u001b[39m\u001b[92m+ DiffResults v1.1.0\u001b[39m\n",
      "  \u001b[90m[b552c78f] \u001b[39m\u001b[92m+ DiffRules v1.15.1\u001b[39m\n",
      "  \u001b[90m[31c24e10] \u001b[39m\u001b[92m+ Distributions v0.25.122\u001b[39m\n",
      "  \u001b[90m[ffbed154] \u001b[39m\u001b[92m+ DocStringExtensions v0.9.5\u001b[39m\n",
      "  \u001b[90m[460bff9d] \u001b[39m\u001b[92m+ ExceptionUnwrapping v0.1.11\u001b[39m\n",
      "  \u001b[90m[e2ba6199] \u001b[39m\u001b[92m+ ExprTools v0.1.10\u001b[39m\n",
      "  \u001b[90m[c87230d0] \u001b[39m\u001b[92m+ FFMPEG v0.4.5\u001b[39m\n",
      "  \u001b[90m[9aa1b823] \u001b[39m\u001b[92m+ FastClosures v0.3.2\u001b[39m\n",
      "  \u001b[90m[5789e2e9] \u001b[39m\u001b[92m+ FileIO v1.17.1\u001b[39m\n",
      "  \u001b[90m[48062228] \u001b[39m\u001b[92m+ FilePathsBase v0.9.24\u001b[39m\n",
      "  \u001b[90m[1a297f60] \u001b[39m\u001b[92m+ FillArrays v1.15.0\u001b[39m\n",
      "  \u001b[90m[53c48c17] \u001b[39m\u001b[92m+ FixedPointNumbers v0.8.5\u001b[39m\n",
      "  \u001b[90m[1fa38f19] \u001b[39m\u001b[92m+ Format v1.3.7\u001b[39m\n",
      "  \u001b[90m[f6369f11] \u001b[39m\u001b[92m+ ForwardDiff v1.3.0\u001b[39m\n",
      "  \u001b[90m[60bf3e95] \u001b[39m\u001b[92m+ GLPK v1.2.1\u001b[39m\n",
      "  \u001b[90m[28b8d3ca] \u001b[39m\u001b[92m+ GR v0.73.18\u001b[39m\n",
      "  \u001b[90m[42e2da0e] \u001b[39m\u001b[92m+ Grisu v1.0.2\u001b[39m\n",
      "  \u001b[90m[cd3eb016] \u001b[39m\u001b[92m+ HTTP v1.10.19\u001b[39m\n",
      "  \u001b[90m[34004b35] \u001b[39m\u001b[92m+ HypergeometricFunctions v0.3.28\u001b[39m\n",
      "  \u001b[90m[7073ff75] \u001b[39m\u001b[92m+ IJulia v1.32.1\u001b[39m\n",
      "  \u001b[90m[842dd82b] \u001b[39m\u001b[92m+ InlineStrings v1.4.5\u001b[39m\n",
      "  \u001b[90m[41ab1584] \u001b[39m\u001b[92m+ InvertedIndices v1.3.1\u001b[39m\n",
      "  \u001b[90m[92d709cd] \u001b[39m\u001b[92m+ IrrationalConstants v0.2.6\u001b[39m\n",
      "  \u001b[90m[82899510] \u001b[39m\u001b[92m+ IteratorInterfaceExtensions v1.0.0\u001b[39m\n",
      "  \u001b[90m[1019f520] \u001b[39m\u001b[92m+ JLFzf v0.1.11\u001b[39m\n",
      "  \u001b[90m[692b3bcd] \u001b[39m\u001b[92m+ JLLWrappers v1.7.1\u001b[39m\n",
      "  \u001b[90m[682c06a0] \u001b[39m\u001b[92m+ JSON v1.3.0\u001b[39m\n",
      "  \u001b[90m[0f8b85d8] \u001b[39m\u001b[92m+ JSON3 v1.14.3\u001b[39m\n",
      "  \u001b[90m[4076af6c] \u001b[39m\u001b[92m+ JuMP v1.29.3\u001b[39m\n",
      "  \u001b[90m[40e66cde] \u001b[39m\u001b[92m+ LDLFactorizations v0.10.1\u001b[39m\n",
      "  \u001b[90m[b964fa9f] \u001b[39m\u001b[92m+ LaTeXStrings v1.4.0\u001b[39m\n",
      "  \u001b[90m[23fbe1c1] \u001b[39m\u001b[92m+ Latexify v0.16.10\u001b[39m\n",
      "  \u001b[90m[5c8ed15e] \u001b[39m\u001b[92m+ LinearOperators v2.11.0\u001b[39m\n",
      "  \u001b[90m[2ab3a3ac] \u001b[39m\u001b[92m+ LogExpFunctions v0.3.29\u001b[39m\n",
      "  \u001b[90m[e6f89c97] \u001b[39m\u001b[92m+ LoggingExtras v1.2.0\u001b[39m\n",
      "  \u001b[90m[1914dd2f] \u001b[39m\u001b[92m+ MacroTools v0.5.16\u001b[39m\n",
      "  \u001b[90m[2621e9c9] \u001b[39m\u001b[92m+ MadNLP v0.8.12\u001b[39m\n",
      "  \u001b[90m[b8f27783] \u001b[39m\u001b[92m+ MathOptInterface v1.46.0\u001b[39m\n",
      "  \u001b[90m[739be429] \u001b[39m\u001b[92m+ MbedTLS v1.1.9\u001b[39m\n",
      "  \u001b[90m[442fdcdd] \u001b[39m\u001b[92m+ Measures v0.3.3\u001b[39m\n",
      "  \u001b[90m[e1d29d7a] \u001b[39m\u001b[92m+ Missings v1.2.0\u001b[39m\n",
      "  \u001b[90m[d8a4904e] \u001b[39m\u001b[92m+ MutableArithmetics v1.6.7\u001b[39m\n",
      "  \u001b[90m[a4795742] \u001b[39m\u001b[92m+ NLPModels v0.21.5\u001b[39m\n",
      "  \u001b[90m[77ba4419] \u001b[39m\u001b[92m+ NaNMath v1.1.3\u001b[39m\n",
      "  \u001b[90m[4d8831e6] \u001b[39m\u001b[92m+ OpenSSL v1.6.0\u001b[39m\n",
      "  \u001b[90m[bac558e1] \u001b[39m\u001b[92m+ OrderedCollections v1.8.1\u001b[39m\n",
      "  \u001b[90m[90014a1f] \u001b[39m\u001b[92m+ PDMats v0.11.36\u001b[39m\n",
      "  \u001b[90m[69de0a69] \u001b[39m\u001b[92m+ Parsers v2.8.3\u001b[39m\n",
      "  \u001b[90m[ccf2f8ad] \u001b[39m\u001b[92m+ PlotThemes v3.3.0\u001b[39m\n",
      "  \u001b[90m[995b91a9] \u001b[39m\u001b[92m+ PlotUtils v1.4.4\u001b[39m\n",
      "  \u001b[90m[91a5bcdd] \u001b[39m\u001b[92m+ Plots v1.41.1\u001b[39m\n",
      "  \u001b[90m[2dfb63ee] \u001b[39m\u001b[92m+ PooledArrays v1.4.3\u001b[39m\n",
      "  \u001b[90m[aea7be01] \u001b[39m\u001b[92m+ PrecompileTools v1.3.3\u001b[39m\n",
      "  \u001b[90m[21216c6a] \u001b[39m\u001b[92m+ Preferences v1.5.0\u001b[39m\n",
      "  \u001b[90m[08abe8d2] \u001b[39m\u001b[92m+ PrettyTables v3.1.0\u001b[39m\n",
      "  \u001b[90m[43287f4e] \u001b[39m\u001b[92m+ PtrArrays v1.3.0\u001b[39m\n",
      "  \u001b[90m[1fd47b50] \u001b[39m\u001b[92m+ QuadGK v2.11.2\u001b[39m\n",
      "  \u001b[90m[3cdcf5f2] \u001b[39m\u001b[92m+ RecipesBase v1.3.4\u001b[39m\n",
      "  \u001b[90m[01d81517] \u001b[39m\u001b[92m+ RecipesPipeline v0.6.12\u001b[39m\n",
      "  \u001b[90m[189a3867] \u001b[39m\u001b[92m+ Reexport v1.2.2\u001b[39m\n",
      "  \u001b[90m[05181044] \u001b[39m\u001b[92m+ RelocatableFolders v1.0.1\u001b[39m\n",
      "  \u001b[90m[ae029012] \u001b[39m\u001b[92m+ Requires v1.3.1\u001b[39m\n",
      "  \u001b[90m[79098fc4] \u001b[39m\u001b[92m+ Rmath v0.9.0\u001b[39m\n",
      "  \u001b[90m[6c6a2e73] \u001b[39m\u001b[92m+ Scratch v1.3.0\u001b[39m\n",
      "  \u001b[90m[91c51154] \u001b[39m\u001b[92m+ SentinelArrays v1.4.8\u001b[39m\n",
      "  \u001b[90m[992d4aef] \u001b[39m\u001b[92m+ Showoff v1.0.3\u001b[39m\n",
      "  \u001b[90m[777ac1f9] \u001b[39m\u001b[92m+ SimpleBufferStream v1.2.0\u001b[39m\n",
      "  \u001b[90m[ff4d7338] \u001b[39m\u001b[92m+ SolverCore v0.3.8\u001b[39m\n",
      "  \u001b[90m[a2af1166] \u001b[39m\u001b[92m+ SortingAlgorithms v1.2.2\u001b[39m\n",
      "  \u001b[90m[276daf66] \u001b[39m\u001b[92m+ SpecialFunctions v2.6.1\u001b[39m\n",
      "  \u001b[90m[860ef19b] \u001b[39m\u001b[92m+ StableRNGs v1.0.4\u001b[39m\n",
      "  \u001b[90m[1e83bf80] \u001b[39m\u001b[92m+ StaticArraysCore v1.4.4\u001b[39m\n",
      "  \u001b[90m[10745b16] \u001b[39m\u001b[92m+ Statistics v1.11.1\u001b[39m\n",
      "  \u001b[90m[82ae8749] \u001b[39m\u001b[92m+ StatsAPI v1.7.1\u001b[39m\n",
      "  \u001b[90m[2913bbd2] \u001b[39m\u001b[92m+ StatsBase v0.34.8\u001b[39m\n",
      "  \u001b[90m[4c63d2b9] \u001b[39m\u001b[92m+ StatsFuns v1.5.2\u001b[39m\n",
      "  \u001b[90m[892a3eda] \u001b[39m\u001b[92m+ StringManipulation v0.4.1\u001b[39m\n",
      "  \u001b[90m[856f2bd8] \u001b[39m\u001b[92m+ StructTypes v1.11.0\u001b[39m\n",
      "  \u001b[90m[ec057cc2] \u001b[39m\u001b[92m+ StructUtils v2.6.0\u001b[39m\n",
      "  \u001b[90m[3783bdb8] \u001b[39m\u001b[92m+ TableTraits v1.0.1\u001b[39m\n",
      "  \u001b[90m[bd369af6] \u001b[39m\u001b[92m+ Tables v1.12.1\u001b[39m\n",
      "  \u001b[90m[62fd8b95] \u001b[39m\u001b[92m+ TensorCore v0.1.1\u001b[39m\n",
      "  \u001b[90m[a759f4b9] \u001b[39m\u001b[92m+ TimerOutputs v0.5.29\u001b[39m\n",
      "  \u001b[90m[3bb67fe8] \u001b[39m\u001b[92m+ TranscodingStreams v0.11.3\u001b[39m\n",
      "  \u001b[90m[5c2747f8] \u001b[39m\u001b[92m+ URIs v1.6.1\u001b[39m\n",
      "  \u001b[90m[1cfade01] \u001b[39m\u001b[92m+ UnicodeFun v0.4.1\u001b[39m\n",
      "  \u001b[90m[41fe7b60] \u001b[39m\u001b[92m+ Unzip v0.2.0\u001b[39m\n",
      "  \u001b[90m[24b76065] \u001b[39m\u001b[92m+ VLDataScienceMachineLearningPackage v0.1.0 `https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl.git#main`\u001b[39m\n",
      "  \u001b[90m[81def892] \u001b[39m\u001b[92m+ VersionParsing v1.3.0\u001b[39m\n",
      "  \u001b[90m[ea10d353] \u001b[39m\u001b[92m+ WeakRefStrings v1.4.2\u001b[39m\n",
      "  \u001b[90m[76eceee3] \u001b[39m\u001b[92m+ WorkerUtilities v1.6.1\u001b[39m\n",
      "  \u001b[90m[c2297ded] \u001b[39m\u001b[92m+ ZMQ v1.5.1\u001b[39m\n",
      "  \u001b[90m[6e34b625] \u001b[39m\u001b[92m+ Bzip2_jll v1.0.9+0\u001b[39m\n",
      "  \u001b[90m[83423d85] \u001b[39m\u001b[92m+ Cairo_jll v1.18.5+0\u001b[39m\n",
      "  \u001b[90m[ee1fde0b] \u001b[39m\u001b[92m+ Dbus_jll v1.16.2+0\u001b[39m\n",
      "  \u001b[90m[2702e6a9] \u001b[39m\u001b[92m+ EpollShim_jll v0.0.20230411+1\u001b[39m\n",
      "  \u001b[90m[2e619515] \u001b[39m\u001b[92m+ Expat_jll v2.7.3+0\u001b[39m\n",
      "  \u001b[90m[b22a6f82] \u001b[39m\u001b[92m+ FFMPEG_jll v8.0.0+0\u001b[39m\n",
      "  \u001b[90m[a3f928ae] \u001b[39m\u001b[92m+ Fontconfig_jll v2.17.1+0\u001b[39m\n",
      "  \u001b[90m[d7e528f0] \u001b[39m\u001b[92m+ FreeType2_jll v2.13.4+0\u001b[39m\n",
      "  \u001b[90m[559328eb] \u001b[39m\u001b[92m+ FriBidi_jll v1.0.17+0\u001b[39m\n",
      "  \u001b[90m[0656b61e] \u001b[39m\u001b[92m+ GLFW_jll v3.4.0+2\u001b[39m\n",
      "  \u001b[90m[e8aa6df9] \u001b[39m\u001b[92m+ GLPK_jll v5.0.1+1\u001b[39m\n",
      "  \u001b[90m[d2c73de3] \u001b[39m\u001b[92m+ GR_jll v0.73.18+0\u001b[39m\n",
      "  \u001b[90m[b0724c58] \u001b[39m\u001b[92m+ GettextRuntime_jll v0.22.4+0\u001b[39m\n",
      "  \u001b[90m[61579ee1] \u001b[39m\u001b[92m+ Ghostscript_jll v9.55.1+0\u001b[39m\n",
      "  \u001b[90m[7746bdde] \u001b[39m\u001b[92m+ Glib_jll v2.86.0+0\u001b[39m\n",
      "  \u001b[90m[3b182d85] \u001b[39m\u001b[92m+ Graphite2_jll v1.3.15+0\u001b[39m\n",
      "  \u001b[90m[2e76f6c2] \u001b[39m\u001b[92m+ HarfBuzz_jll v8.5.1+0\u001b[39m\n",
      "  \u001b[90m[aacddb02] \u001b[39m\u001b[92m+ JpegTurbo_jll v3.1.3+0\u001b[39m\n",
      "  \u001b[90m[c1c5ebd0] \u001b[39m\u001b[92m+ LAME_jll v3.100.3+0\u001b[39m\n",
      "  \u001b[90m[88015f11] \u001b[39m\u001b[92m+ LERC_jll v4.0.1+0\u001b[39m\n",
      "  \u001b[90m[1d63c593] \u001b[39m\u001b[92m+ LLVMOpenMP_jll v18.1.8+0\u001b[39m\n",
      "  \u001b[90m[dd4b983a] \u001b[39m\u001b[92m+ LZO_jll v2.10.3+0\u001b[39m\n",
      "  \u001b[90m[e9f186c6] \u001b[39m\u001b[92m+ Libffi_jll v3.4.7+0\u001b[39m\n",
      "  \u001b[90m[7e76a0d4] \u001b[39m\u001b[92m+ Libglvnd_jll v1.7.1+1\u001b[39m\n",
      "  \u001b[90m[94ce4f54] \u001b[39m\u001b[92m+ Libiconv_jll v1.18.0+0\u001b[39m\n",
      "  \u001b[90m[4b2f31a3] \u001b[39m\u001b[92m+ Libmount_jll v2.41.2+0\u001b[39m\n",
      "  \u001b[90m[89763e89] \u001b[39m\u001b[92m+ Libtiff_jll v4.7.2+0\u001b[39m\n",
      "  \u001b[90m[38a345b3] \u001b[39m\u001b[92m+ Libuuid_jll v2.41.2+0\u001b[39m\n",
      "  \u001b[90m[c8ffd9c3] \u001b[39m\u001b[92m+ MbedTLS_jll v2.28.10+0\u001b[39m\n",
      "  \u001b[90m[e7412a2a] \u001b[39m\u001b[92m+ Ogg_jll v1.3.6+0\u001b[39m\n",
      "  \u001b[90m[efe28fd5] \u001b[39m\u001b[92m+ OpenSpecFun_jll v0.5.6+0\u001b[39m\n",
      "  \u001b[90m[91d4177d] \u001b[39m\u001b[92m+ Opus_jll v1.5.2+0\u001b[39m\n",
      "  \u001b[90m[36c8627f] \u001b[39m\u001b[92m+ Pango_jll v1.57.0+0\u001b[39m\n",
      "\u001b[33m‚åÖ\u001b[39m \u001b[90m[30392449] \u001b[39m\u001b[92m+ Pixman_jll v0.44.2+0\u001b[39m\n",
      "  \u001b[90m[c0090381] \u001b[39m\u001b[92m+ Qt6Base_jll v6.8.2+2\u001b[39m\n",
      "  \u001b[90m[629bc702] \u001b[39m\u001b[92m+ Qt6Declarative_jll v6.8.2+1\u001b[39m\n",
      "  \u001b[90m[ce943373] \u001b[39m\u001b[92m+ Qt6ShaderTools_jll v6.8.2+1\u001b[39m\n",
      "  \u001b[90m[e99dba38] \u001b[39m\u001b[92m+ Qt6Wayland_jll v6.8.2+2\u001b[39m\n",
      "  \u001b[90m[f50d1b31] \u001b[39m\u001b[92m+ Rmath_jll v0.5.1+0\u001b[39m\n",
      "  \u001b[90m[a44049a8] \u001b[39m\u001b[92m+ Vulkan_Loader_jll v1.3.243+0\u001b[39m\n",
      "  \u001b[90m[a2964d1f] \u001b[39m\u001b[92m+ Wayland_jll v1.24.0+0\u001b[39m\n",
      "  \u001b[90m[ffd25f8a] \u001b[39m\u001b[92m+ XZ_jll v5.8.1+0\u001b[39m\n",
      "  \u001b[90m[f67eecfb] \u001b[39m\u001b[92m+ Xorg_libICE_jll v1.1.2+0\u001b[39m\n",
      "  \u001b[90m[c834827a] \u001b[39m\u001b[92m+ Xorg_libSM_jll v1.2.6+0\u001b[39m\n",
      "  \u001b[90m[4f6342f7] \u001b[39m\u001b[92m+ Xorg_libX11_jll v1.8.12+0\u001b[39m\n",
      "  \u001b[90m[0c0b7dd1] \u001b[39m\u001b[92m+ Xorg_libXau_jll v1.0.13+0\u001b[39m\n",
      "  \u001b[90m[935fb764] \u001b[39m\u001b[92m+ Xorg_libXcursor_jll v1.2.4+0\u001b[39m\n",
      "  \u001b[90m[a3789734] \u001b[39m\u001b[92m+ Xorg_libXdmcp_jll v1.1.6+0\u001b[39m\n",
      "  \u001b[90m[1082639a] \u001b[39m\u001b[92m+ Xorg_libXext_jll v1.3.7+0\u001b[39m\n",
      "  \u001b[90m[d091e8ba] \u001b[39m\u001b[92m+ Xorg_libXfixes_jll v6.0.2+0\u001b[39m\n",
      "  \u001b[90m[a51aa0fd] \u001b[39m\u001b[92m+ Xorg_libXi_jll v1.8.3+0\u001b[39m\n",
      "  \u001b[90m[d1454406] \u001b[39m\u001b[92m+ Xorg_libXinerama_jll v1.1.6+0\u001b[39m\n",
      "  \u001b[90m[ec84b674] \u001b[39m\u001b[92m+ Xorg_libXrandr_jll v1.5.5+0\u001b[39m\n",
      "  \u001b[90m[ea2f1a96] \u001b[39m\u001b[92m+ Xorg_libXrender_jll v0.9.12+0\u001b[39m\n",
      "  \u001b[90m[c7cfdc94] \u001b[39m\u001b[92m+ Xorg_libxcb_jll v1.17.1+0\u001b[39m\n",
      "  \u001b[90m[cc61e674] \u001b[39m\u001b[92m+ Xorg_libxkbfile_jll v1.1.3+0\u001b[39m\n",
      "  \u001b[90m[e920d4aa] \u001b[39m\u001b[92m+ Xorg_xcb_util_cursor_jll v0.1.6+0\u001b[39m\n",
      "  \u001b[90m[12413925] \u001b[39m\u001b[92m+ Xorg_xcb_util_image_jll v0.4.1+0\u001b[39m\n",
      "  \u001b[90m[2def613f] \u001b[39m\u001b[92m+ Xorg_xcb_util_jll v0.4.1+0\u001b[39m\n",
      "  \u001b[90m[975044d2] \u001b[39m\u001b[92m+ Xorg_xcb_util_keysyms_jll v0.4.1+0\u001b[39m\n",
      "  \u001b[90m[0d47668e] \u001b[39m\u001b[92m+ Xorg_xcb_util_renderutil_jll v0.3.10+0\u001b[39m\n",
      "  \u001b[90m[c22f9ab0] \u001b[39m\u001b[92m+ Xorg_xcb_util_wm_jll v0.4.2+0\u001b[39m\n",
      "  \u001b[90m[35661453] \u001b[39m\u001b[92m+ Xorg_xkbcomp_jll v1.4.7+0\u001b[39m\n",
      "  \u001b[90m[33bec58e] \u001b[39m\u001b[92m+ Xorg_xkeyboard_config_jll v2.44.0+0\u001b[39m\n",
      "  \u001b[90m[c5fb5394] \u001b[39m\u001b[92m+ Xorg_xtrans_jll v1.6.0+0\u001b[39m\n",
      "  \u001b[90m[8f1865be] \u001b[39m\u001b[92m+ ZeroMQ_jll v4.3.6+0\u001b[39m\n",
      "  \u001b[90m[3161d3a3] \u001b[39m\u001b[92m+ Zstd_jll v1.5.7+1\u001b[39m\n",
      "  \u001b[90m[35ca27e7] \u001b[39m\u001b[92m+ eudev_jll v3.2.14+0\u001b[39m\n",
      "  \u001b[90m[214eeab7] \u001b[39m\u001b[92m+ fzf_jll v0.61.1+0\u001b[39m\n",
      "  \u001b[90m[a4ae2306] \u001b[39m\u001b[92m+ libaom_jll v3.13.1+0\u001b[39m\n",
      "  \u001b[90m[0ac62f75] \u001b[39m\u001b[92m+ libass_jll v0.17.4+0\u001b[39m\n",
      "  \u001b[90m[1183f4f0] \u001b[39m\u001b[92m+ libdecor_jll v0.2.2+0\u001b[39m\n",
      "  \u001b[90m[2db6ffa8] \u001b[39m\u001b[92m+ libevdev_jll v1.13.4+0\u001b[39m\n",
      "  \u001b[90m[f638f0a6] \u001b[39m\u001b[92m+ libfdk_aac_jll v2.0.4+0\u001b[39m\n",
      "  \u001b[90m[36db933b] \u001b[39m\u001b[92m+ libinput_jll v1.28.1+0\u001b[39m\n",
      "  \u001b[90m[b53b4c65] \u001b[39m\u001b[92m+ libpng_jll v1.6.50+0\u001b[39m\n",
      "  \u001b[90m[a9144af2] \u001b[39m\u001b[92m+ libsodium_jll v1.0.21+0\u001b[39m\n",
      "  \u001b[90m[f27f6e37] \u001b[39m\u001b[92m+ libvorbis_jll v1.3.8+0\u001b[39m\n",
      "  \u001b[90m[009596ad] \u001b[39m\u001b[92m+ mtdev_jll v1.1.7+0\u001b[39m\n",
      "  \u001b[90m[1270edf5] \u001b[39m\u001b[92m+ x264_jll v10164.0.1+0\u001b[39m\n",
      "  \u001b[90m[dfaa095f] \u001b[39m\u001b[92m+ x265_jll v4.1.0+0\u001b[39m\n",
      "  \u001b[90m[d8fb68d0] \u001b[39m\u001b[92m+ xkbcommon_jll v1.9.2+0\u001b[39m\n",
      "  \u001b[90m[0dad84c5] \u001b[39m\u001b[92m+ ArgTools v1.1.2\u001b[39m\n",
      "  \u001b[90m[56f22d72] \u001b[39m\u001b[92m+ Artifacts v1.11.0\u001b[39m\n",
      "  \u001b[90m[2a0f44e3] \u001b[39m\u001b[92m+ Base64 v1.11.0\u001b[39m\n",
      "  \u001b[90m[ade2ca70] \u001b[39m\u001b[92m+ Dates v1.11.0\u001b[39m\n",
      "  \u001b[90m[f43a241f] \u001b[39m\u001b[92m+ Downloads v1.6.0\u001b[39m\n",
      "  \u001b[90m[7b1f6079] \u001b[39m\u001b[92m+ FileWatching v1.11.0\u001b[39m\n",
      "  \u001b[90m[9fa8497b] \u001b[39m\u001b[92m+ Future v1.11.0\u001b[39m\n",
      "  \u001b[90m[b77e0a4c] \u001b[39m\u001b[92m+ InteractiveUtils v1.11.0\u001b[39m\n",
      "  \u001b[90m[ac6e5ff7] \u001b[39m\u001b[92m+ JuliaSyntaxHighlighting v1.12.0\u001b[39m\n",
      "  \u001b[90m[b27032c2] \u001b[39m\u001b[92m+ LibCURL v0.6.4\u001b[39m\n",
      "  \u001b[90m[76f85450] \u001b[39m\u001b[92m+ LibGit2 v1.11.0\u001b[39m\n",
      "  \u001b[90m[8f399da3] \u001b[39m\u001b[92m+ Libdl v1.11.0\u001b[39m\n",
      "  \u001b[90m[37e2e46d] \u001b[39m\u001b[93m~ LinearAlgebra ‚áí v1.12.0\u001b[39m\n",
      "  \u001b[90m[56ddb016] \u001b[39m\u001b[92m+ Logging v1.11.0\u001b[39m\n",
      "  \u001b[90m[d6f4376e] \u001b[39m\u001b[92m+ Markdown v1.11.0\u001b[39m\n",
      "  \u001b[90m[a63ad114] \u001b[39m\u001b[92m+ Mmap v1.11.0\u001b[39m\n",
      "  \u001b[90m[ca575930] \u001b[39m\u001b[92m+ NetworkOptions v1.3.0\u001b[39m\n",
      "  \u001b[90m[44cfe95a] \u001b[39m\u001b[92m+ Pkg v1.12.0\u001b[39m\n",
      "  \u001b[90m[de0858da] \u001b[39m\u001b[92m+ Printf v1.11.0\u001b[39m\n",
      "  \u001b[90m[9abbd945] \u001b[39m\u001b[92m+ Profile v1.11.0\u001b[39m\n",
      "  \u001b[90m[3fa0cd96] \u001b[39m\u001b[92m+ REPL v1.11.0\u001b[39m\n",
      "  \u001b[90m[9a3f8284] \u001b[39m\u001b[92m+ Random v1.11.0\u001b[39m\n",
      "  \u001b[90m[ea8e919c] \u001b[39m\u001b[92m+ SHA v0.7.0\u001b[39m\n",
      "  \u001b[90m[9e88b42a] \u001b[39m\u001b[92m+ Serialization v1.11.0\u001b[39m\n",
      "  \u001b[90m[6462fe0b] \u001b[39m\u001b[92m+ Sockets v1.11.0\u001b[39m\n",
      "  \u001b[90m[2f01184e] \u001b[39m\u001b[92m+ SparseArrays v1.12.0\u001b[39m\n",
      "  \u001b[90m[f489334b] \u001b[39m\u001b[92m+ StyledStrings v1.11.0\u001b[39m\n",
      "  \u001b[90m[4607b0f0] \u001b[39m\u001b[92m+ SuiteSparse\u001b[39m\n",
      "  \u001b[90m[fa267f1f] \u001b[39m\u001b[92m+ TOML v1.0.3\u001b[39m\n",
      "  \u001b[90m[a4e569a6] \u001b[39m\u001b[92m+ Tar v1.10.0\u001b[39m\n",
      "  \u001b[90m[8dfed614] \u001b[39m\u001b[93m~ Test ‚áí v1.11.0\u001b[39m\n",
      "  \u001b[90m[cf7118a7] \u001b[39m\u001b[92m+ UUIDs v1.11.0\u001b[39m\n",
      "  \u001b[90m[4ec0a83e] \u001b[39m\u001b[92m+ Unicode v1.11.0\u001b[39m\n",
      "  \u001b[90m[e66e0078] \u001b[39m\u001b[92m+ CompilerSupportLibraries_jll v1.3.0+1\u001b[39m\n",
      "  \u001b[90m[781609d7] \u001b[39m\u001b[92m+ GMP_jll v6.3.0+2\u001b[39m\n",
      "  \u001b[90m[deac9b47] \u001b[39m\u001b[92m+ LibCURL_jll v8.11.1+1\u001b[39m\n",
      "  \u001b[90m[e37daf67] \u001b[39m\u001b[92m+ LibGit2_jll v1.9.0+0\u001b[39m\n",
      "  \u001b[90m[29816b5a] \u001b[39m\u001b[92m+ LibSSH2_jll v1.11.3+1\u001b[39m\n",
      "  \u001b[90m[14a3606d] \u001b[39m\u001b[92m+ MozillaCACerts_jll v2025.5.20\u001b[39m\n",
      "  \u001b[90m[4536629a] \u001b[39m\u001b[92m+ OpenBLAS_jll v0.3.29+0\u001b[39m\n",
      "  \u001b[90m[05823500] \u001b[39m\u001b[92m+ OpenLibm_jll v0.8.7+0\u001b[39m\n",
      "  \u001b[90m[458c3c95] \u001b[39m\u001b[92m+ OpenSSL_jll v3.5.1+0\u001b[39m\n",
      "  \u001b[90m[efcefdf7] \u001b[39m\u001b[92m+ PCRE2_jll v10.44.0+1\u001b[39m\n",
      "  \u001b[90m[bea87d4a] \u001b[39m\u001b[92m+ SuiteSparse_jll v7.8.3+2\u001b[39m\n",
      "  \u001b[90m[83775a58] \u001b[39m\u001b[92m+ Zlib_jll v1.3.1+2\u001b[39m\n",
      "  \u001b[90m[8e850b90] \u001b[39m\u001b[92m+ libblastrampoline_jll v5.15.0+0\u001b[39m\n",
      "  \u001b[90m[8e850ede] \u001b[39m\u001b[92m+ nghttp2_jll v1.64.0+1\u001b[39m\n",
      "  \u001b[90m[3f19e933] \u001b[39m\u001b[92m+ p7zip_jll v17.5.0+2\u001b[39m\n",
      "\u001b[36m\u001b[1m        Info\u001b[22m\u001b[39m Packages marked with \u001b[33m‚åÖ\u001b[39m have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated -m`\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m packages...\n",
      "   6198.2 ms\u001b[32m  ‚úì \u001b[39m\u001b[90mJSON\u001b[39m\n",
      "   3925.6 ms\u001b[32m  ‚úì \u001b[39m\u001b[90mBenchmarkTools\u001b[39m\n",
      "   4556.6 ms\u001b[32m  ‚úì \u001b[39m\u001b[90mConda\u001b[39m\n",
      "  64272.5 ms\u001b[32m  ‚úì \u001b[39m\u001b[90mMathOptInterface\u001b[39m\n",
      "   6338.7 ms\u001b[32m  ‚úì \u001b[39mMadNLP ‚Üí MadNLPMOI\n",
      "  10467.2 ms\u001b[32m  ‚úì \u001b[39m\u001b[90mGLPK\u001b[39m\n",
      "  94350.8 ms\u001b[32m  ‚úì \u001b[39mPlots\n",
      "   7441.8 ms\u001b[32m  ‚úì \u001b[39mPlots ‚Üí FileIOExt\n",
      "  13861.2 ms\u001b[32m  ‚úì \u001b[39mPlots ‚Üí IJuliaExt\n",
      "  42536.2 ms\u001b[32m  ‚úì \u001b[39mJuMP\n",
      "  21601.1 ms\u001b[32m  ‚úì \u001b[39mVLDataScienceMachineLearningPackage\n",
      "  11 dependencies successfully precompiled in 143 seconds. 245 already precompiled.\n",
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646`\n",
      "\u001b[36m\u001b[1m     Project\u001b[22m\u001b[39m No packages added to or removed from `C:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646\\Project.toml`\n",
      "\u001b[36m\u001b[1m    Manifest\u001b[22m\u001b[39m No packages added to or removed from `C:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `C:\\Users\\Bhargav\\.julia\\registries\\General.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl.git`\n",
      "\u001b[36m\u001b[1m     Project\u001b[22m\u001b[39m No packages added to or removed from `C:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646\\Project.toml`\n",
      "\u001b[36m\u001b[1m    Manifest\u001b[22m\u001b[39m No packages added to or removed from `C:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646\\Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "include(\"Include-student.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591de334",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e777da94-8a5f-45e6-bf2e-ec8abee6fee1",
   "metadata": {},
   "source": [
    "### Helper Function: Cobb-Douglas Utility\n",
    "\n",
    "We need a utility function to evaluate the desirability of different combinations of apples and oranges. The utility function `U(...)` computes the utility of combinations of apples and oranges using a Cobb-Douglas utility model. A tuple holding the number of `(apples, oranges)`, i.e., the combinations of objects we are searching over, and the $\\alpha$-vector (preferences) are passed as arguments; the `utility` value is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca18d71-b870-4cd1-a70b-821265f5de38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function U(x::Tuple{Int,Int}, Œ±::Array{Float64,1})::Float64\n",
    "    \n",
    "    # get the apples, and oranges \n",
    "    apples = x[1];\n",
    "    oranges = x[2];\n",
    "    \n",
    "    # compute the objective -\n",
    "    utility = (apples^Œ±[1])*(oranges^Œ±[2]);\n",
    "    \n",
    "    # return -\n",
    "    return utility;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e49acf",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca9647f-0576-4a9e-81bf-b01c0ec4ade0",
   "metadata": {},
   "source": [
    "## Problem 1: Compute the optimal number of Apples and Oranges to purchase\n",
    "In this problem, we compute the optimal number of apples and oranges to purchase given a budget constraint using nonlinear programming, i.e., we maximize the utility function subject to the budget constraint.\n",
    "\n",
    "### Problem Statement\n",
    "Use a Cobb-Douglas utility function combined with a budget constraint to compute the optimal combination of apples and oranges that gives the maximum utility for the available budget. The Cobb-Douglas utility function for a collection of objects $x_{1},\\cdots,x_{n}$ is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{maximize } U(x_{1},\\cdots,x_{n}) &= \\prod_{i=1}^{n}x_{i}^{\\alpha_{i}}\\\\\n",
    "\\text{subject to } \\sum_{i=1}^{n}p_{i}x_{i} &\\leq B\\\\\n",
    "x_{i} &\\geq 0, \\quad i=1,\\cdots,n\n",
    "\\end{align*}\n",
    "$$\n",
    "where the $\\alpha_{i}\\geq{0}$ parameters are preference coefficients associated with each object, $p_{i}$ is the unit price of object $i$, $x_{i}$ are the quantities of each object, and $B$ is the total budget available for purchasing the objects.\n",
    "\n",
    "> __Setup__:\n",
    ">\n",
    "> For this problem, let's assume we have the following parameters:\n",
    "> * The preference coefficient vector $\\alpha = (0.55,0.45)$ and the total budget $B$ = `50 USD`\n",
    "> * The unit price of an `apple` is `0.98 USD` and the unit price of an `orange` is `1.49 USD`\n",
    "> * Let `apples` be index `1` and `oranges` be index `2`\n",
    "> * Assume the bounds run from `0` to $B/p_i$ for each good $i$ and the initial guess is `0.1*ones(2)`.\n",
    "\n",
    "We'll use [the `build(...)` method](src/Factory.jl) to construct an instance of [the `MySimpleCobbDouglasChoiceProblem` type](src/Types.jl) holding the parameters in the `base` variable. We'll then pass `base` to [the `mysolve(...)` function](src/Compute.jl) and set the return to the variable `base_solution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28507283-c742-476c-a301-404d18c21598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is MadNLP version v0.8.12, running with umfpack\n",
      "\n",
      "Number of nonzeros in constraint Jacobian............:        2\n",
      "Number of nonzeros in Lagrangian Hessian.............:        3\n",
      "\n",
      "Total number of variables............................:        2\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        2\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        1\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        1\n",
      "\n",
      "iter    objective    inf_pr   inf_du inf_compl lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0 -1.0000000e-01 0.00e+00 4.71e-01 5.09e+01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1 -2.4856725e-01 1.22e-15 3.45e-01 1.78e+01  -1.0 3.64e-01    -  6.52e-01 1.00e+00h  1\n",
      "   2 -5.9438981e+00 0.00e+00 2.37e-01 1.48e+01  -1.0 1.38e+01    -  6.40e-02 1.00e+00f  1\n",
      "   3 -1.9164790e+01 0.00e+00 1.91e-01 7.36e+00  -1.0 3.16e+01    -  3.86e-01 1.00e+00f  1\n",
      "   4 -2.1137355e+01 1.42e-14 2.67e-02 1.26e+00  -1.0 5.34e+00    -  9.89e-01 8.58e-01f  1\n",
      "   5 -2.1126904e+01 7.11e-15 2.77e-03 1.82e-01  -1.0 1.95e+00    -  1.00e+00 1.00e+00h  1\n",
      "   6 -2.1230793e+01 0.00e+00 1.14e-05 4.77e-03  -2.5 2.54e-01    -  1.00e+00 1.00e+00h  1\n",
      "   7 -2.1232635e+01 0.00e+00 1.26e-08 1.52e-04  -3.8 7.73e-03    -  1.00e+00 1.00e+00h  1\n",
      "   8 -2.1232785e+01 0.00e+00 1.07e-11 4.32e-09  -8.6 3.53e-04    -  1.00e+00 1.00e+00h  1\n",
      "\n",
      "Number of Iterations....: 8\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:  -2.1232784729120219e+01   -2.1232784729120219e+01\n",
      "Dual infeasibility......:   1.0708101072509635e-11    1.0708101072509635e-11\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   4.3219237632533999e-09    4.3219237632533999e-09\n",
      "Overall NLP error.......:   4.3219237632533999e-09    4.3219237632533999e-09\n",
      "\n",
      "Number of objective function evaluations             = 9\n",
      "Number of objective gradient evaluations             = 9\n",
      "Number of constraint evaluations                     = 9\n",
      "Number of constraint Jacobian evaluations            = 1\n",
      "Number of Lagrangian Hessian evaluations             = 8\n",
      "Total wall-clock secs in solver (w/o fun. eval./lin. alg.)  = 35.443\n",
      "Total wall-clock secs in linear solver                      =  0.070\n",
      "Total wall-clock secs in NLP function evaluations           = 10.809\n",
      "Total wall-clock secs                                       = 46.321\n",
      "\n",
      "EXIT: Optimal Solution Found (tol = 1.0e-08).\n"
     ]
    }
   ],
   "source": [
    "base_solution, Œ±, c, B = let \n",
    "\n",
    "    # initialize -\n",
    "    Œ± = [0.55, 0.45]; # coefficients\n",
    "    c = [0.98, 1.49]; # price of x1 and x2\n",
    "    total_budget = 50.0; # how much money we have to spend\n",
    "    base_solution = nothing;\n",
    "\n",
    "    # build my problem object -\n",
    "    model = nothing;\n",
    "    #throw(ErrorException(\"Ooooops! You need to build the MySimpleCobbDouglasChoiceProblem model here!\"));\n",
    "\n",
    "    #\n",
    "    base = build(MySimpleCobbDouglasChoiceProblem, (\n",
    "    \n",
    "        initial = 0.1*ones(2),\n",
    "        Œ± = Œ±, \n",
    "        c = c, \n",
    "        I = total_budget, \n",
    "    \n",
    "        #setting constraints\n",
    "        bounds = [\n",
    "            0.0 total_budget/c[1]; # L U\n",
    "            0.0 total_budget/c[2]; # L U\n",
    "        ]\n",
    "    ));\n",
    "\n",
    "    \n",
    "\n",
    "    # COMPLETED: call the solve function. This will return a dictionary with data about the solution\n",
    "    #throw(ErrorException(\"Ooooops! You need to call the mysolve(...) function here!\"));\n",
    "    \n",
    "    base_solution = mysolve(base);\n",
    "\n",
    "    # return -\n",
    "    base_solution, Œ±, c, total_budget;\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caaeb24b-0d4f-491e-83b3-ee11e9c00190",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 3 entries:\n",
       "  \"argmax\"          => [28.0612, 15.1007]\n",
       "  \"budget\"          => 50.0\n",
       "  \"objective_value\" => 21.2328"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6985ab-a6ef-4b4f-a320-371c140bde07",
   "metadata": {},
   "source": [
    "What is the optimal combination of apples and oranges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54f99fa0-70e2-4969-80f4-d5eeb4fd9ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal: (apples, oranges) = (28,15)\n"
     ]
    }
   ],
   "source": [
    "optimal_apples = base_solution[\"argmax\"][1] |> x-> round(x,digits=0) |> Int\n",
    "optimal_oranges = base_solution[\"argmax\"][2] |> x-> round(x,digits=0) |> Int\n",
    "println(\"Optimal: (apples, oranges) = ($(optimal_apples),$(optimal_oranges))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e208e",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939af937-2f6d-4a17-add7-e50909c87961",
   "metadata": {},
   "source": [
    "## Problem 2: Solve the Apples and Oranges problem as an MDP\n",
    "Now, we solve the apples versus oranges problem as a Markov Decision Process (MDP). \n",
    "\n",
    "> __What are we going to do?__ \n",
    ">\n",
    "> Here are the subtasks associated with solving the apples versus oranges problem as an MDP:\n",
    "> * __Task 1__: Set up a $30\\times{30}$ grid, encoded as an instance of the `MyRectangularGridWorldModel` type.\n",
    ">   * `TODO`: Add a terminal state at the optimal combination of apples and oranges computed from Problem 1. Set the reward for this state as the optimal _integer_ fitness value calculated using the `U(...)` function defined above.\n",
    ">   * `TODO`: Add the optimal combination of apples and oranges computed from Problem 1 to the `absorbing_state_set`.\n",
    "> * __Task 2__: Use your `MyRectangularGridWorldModel` instance to generate the components of the MDP, namely, the reward function (or array) $R(s, a)$ and the model of the physics of the world in the transition function (or array) $T(s, s^{\\prime}, a)$.\n",
    ">    * `TODO`: Modify the $R[s,a]$ array code from lecture `L11d` so that it uses the `U(...)` function for its values. This is a type of reward shaping, as we use the utility function model to give the agent some hints along the way.\n",
    ">    * `TODO`: Modify the $R[s,a]$ array to describe a _soft wall_, i.e., a region where the budget constraint is violated. Unlike a hard wall that makes states unreachable, a soft wall assigns a penalty but still allows the agent to explore these states. Set the wall penalty as `-1000` and allow up to a `1 USD` violation of the budget constraint before applying the penalty.\n",
    "> * __Task 3__: Use value iteration to estimate the optimal value function $U^{\\star}(s)$. \n",
    ">    * `TODO`: For your choice of the $(\\gamma,k_{\\max},\\epsilon)$ hyperparameters, extract the action-value function $Q(s, a)$ from the optimal value function $U^{\\star}(s)$ and compute the optimal navigation policy $\\pi^{\\star}(s)$ from $Q(s,a)$.\n",
    "\n",
    "Let's implement these tasks step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08abf7f-ed7d-4a72-be89-ddb403e1bc06",
   "metadata": {},
   "source": [
    "### Task 1: Build the Apples and Oranges world model\n",
    "In this task, we build the apples and oranges world model as a rectangular grid world. We encode the rectangular grid world as an instance of the `MyRectangularGridWorldModel` model, which we construct using a `build(...)` method. \n",
    "\n",
    "First, let's set up the data for the apples and oranges world, i.e., set up the states, actions, and rewards, and then construct the world model. \n",
    "* `TODO`: Set values for the `number_of_rows` and `number_of_cols` variables, the `nactions` available to the agent, and the discount factor $\\gamma$.\n",
    "* `TODO`: Compute the number of states and set up the state set $\\mathcal{S}$ and the action set $\\mathcal{A}$.\n",
    "\n",
    "Update any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b80a2-e389-49ed-ae8b-8e69b164d4a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rows = 30 # COMPLETED: set the number of rows in the grid\n",
    "number_of_cols = 30 # COMPLETED: set the number of columns in the grid\n",
    "nactions = 4; # COMPLETED: set the number of actions available to the agent (up, down, left, right)\n",
    "nstates = (number_of_rows*number_of_cols); # COMPLETED: set this value: this is the dimension of the state space\n",
    "ùíÆ = range(1,stop=nstates,step=1) |> collect; # we number the states from 1 to nstates\n",
    "ùíú = range(1,stop=nactions,step=1) |> collect; # we number the actions from 1 to nactions\n",
    "Œ≥ = 0.95; # set the discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ba399-75e0-4be9-88a9-e973151aa607",
   "metadata": {},
   "source": [
    "Next, set up a description of the rewards, the `rewards::Dict{Tuple{Int,Int}, Float64}` dictionary, which maps the $(x,y)$-coordinates to a reward value. \n",
    "  * `TODO`: Add a terminal state at the optimal combination of apples and oranges computed from Problem 1. Set the reward for this state to be the optimal _integer_ fitness value (reported above), using the `U(...)` function.\n",
    "  * `TODO`: Add the optimal combination of apples and oranges computed from Problem 1 to the `absorbing_state_set::Set{Tuple{Int,Int}}.` If we arrive at an absorbing state, we stay there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a53a314-6fa1-4ac3-addd-36b1aad0076c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_objective_value = U((optimal_apples, optimal_oranges), Œ±)\n",
    "rewards = Dict{Tuple{Int,Int}, Float64}()\n",
    "rewards[(optimal_apples, optimal_oranges)] = my_objective_value;\n",
    "\n",
    "# setup set of absorbing states -\n",
    "absorbing_state_set = Set{Tuple{Int,Int}}()\n",
    "push!(absorbing_state_set, (optimal_apples, optimal_oranges));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc44527-806d-442a-b915-42a81d06890d",
   "metadata": {},
   "source": [
    "Finally, build an instance of the `MyRectangularGridWorldModel` type, which models the grid world. \n",
    "\n",
    "Pass in the number of rows `nrows`, number of cols `ncols`, and our initial reward description in the `rewards` field into [the `build(...)` method](src/Factory.jl). Save the world model instance to the `world::MyRectangularGridWorldModel` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a91e120-7ca3-48cf-8ee3-ff62ae899c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world = build(MyRectangularGridWorldModel, \n",
    "    (nrows = number_of_rows, ncols = number_of_cols, rewards = rewards));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18b2d22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid world created with 30√ó30 = 900 states\n",
      "Terminal state set at: (apples, oranges) = (28, 15)\n"
     ]
    }
   ],
   "source": [
    "println(\"Grid world created with $(world.number_of_rows)√ó$(world.number_of_cols) = $nstates states\")\n",
    "println(\"Terminal state set at: (apples, oranges) = ($(optimal_apples), $(optimal_oranges))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763bbac2",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf02232-2971-48a3-a8a1-90e5619d1299",
   "metadata": {},
   "source": [
    "### Task 2: Generate the components of the MDP problem\n",
    "In this task, we generate the components of the MDP problem from our `MyRectangularGridWorldModel` instance. The MDP problem requires the reward function (or array) $R(s, a)$ and the transition function (or array) $T(s, s^{\\prime}, a)$. Let's construct these from our grid world model instance, starting with the $R(s, a)$ reward function.\n",
    "\n",
    "#### Rewards $R(s,a)$\n",
    "We'll encode the reward function as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ array, which holds the __immediate__ reward values for being in state $s\\in\\mathcal{S}$ and taking action $a\\in\\mathcal{A}$. After initializing the reward `R::Array{Float64,2}` array with zeros, populate the non-zero values of $R(s, a)$ using a nested [for loop](https://docs.julialang.org/en/v1/base/base/#for). During each outer loop iteration, we select a state $s\\in\\mathcal{S}$, and the inner loop iterates over actions $a\\in\\mathcal{A}$.\n",
    "\n",
    "For each state `s` and action `a` with corresponding move $\\Delta$:\n",
    "* Compute the new position resulting from implementing action `a` and store this in the `new_position` variable. If the `new_position`$\\in\\mathcal{S}$ is in our initial `rewards` dictionary, we use that reward value. If we are still in the world but not in a special location, we set the reward to `-1`. If `new_position`$\\notin\\mathcal{S}$, i.e., the `new_position` is a space outside the grid, we set a penalty of `-50000.0`.\n",
    "\n",
    "#### Modifications\n",
    "The implementation below will be similar to lab `L11d`, but with a few modifications:\n",
    "* `TODO`: Modify the $R[s,a]$ array from `L11d` so that it uses the `U(...)` function for the default values. This is a type of reward shaping.\n",
    "* `TODO`: Modify the $R[s, a]$ array to describe a `soft wall`, i.e., a region where the budget constraint is violated. Set the `wall` penalty as `-1000`. Allow up to a `1 USD` violation of the budget constraint. The off-the-grid penalty is set to `-50000.0`.\n",
    "\n",
    "Go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4885706",
   "metadata": {},
   "outputs": [],
   "source": [
    "R, soft_wall_set = let\n",
    "\n",
    "    # initialize -\n",
    "    total_budget = B; # alias the total budget\n",
    "    R = zeros(nstates, nactions);\n",
    "    fill!(R, 0.0)\n",
    "    off_grid_penalty = -50000.0; # this is the penalty for going off the grid\n",
    "\n",
    "    for s ‚àà ùíÆ\n",
    "        for a ‚àà ùíú\n",
    "            \n",
    "            Œî = world.moves[a];\n",
    "            current_position = world.coordinates[s]\n",
    "            new_position =  current_position .+ Œî\n",
    "            \n",
    "            #COMPLETED: You need to complete the implementation of R matrix here\n",
    "            if (haskey(world.states, new_position) == true)\n",
    "                if (haskey(rewards, new_position) == true)\n",
    "                    R[s,a] = rewards[new_position];\n",
    "                else\n",
    "                    R[s,a] = U(new_position, Œ±);\n",
    "                end\n",
    "            else\n",
    "                R[s,a] = off_grid_penalty; # we are off the grid, big negative penalty\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # setup soft walls -\n",
    "    soft_wall_set = Set{Tuple{Int,Int}}();\n",
    "    for s ‚àà ùíÆ\n",
    "        \n",
    "        # get the position -\n",
    "        current_position = world.coordinates[s]\n",
    "        \n",
    "        # TODO: current_position violate the budget? (with a 1 USD grace)?\n",
    "        # TODO: Hint: think about this like a penalty function\n",
    "        # if yes, store this position in the soft_wall_set\n",
    "    end\n",
    "\n",
    "    for s ‚àà ùíÆ\n",
    "        current_position = world.coordinates[s]\n",
    "        for a ‚àà ùíú\n",
    "            Œî = world.moves[a];\n",
    "            new_position =  current_position .+ Œî\n",
    "            \n",
    "            if (in(new_position, soft_wall_set) == true)\n",
    "                R[s,a] = -1000.0  \n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    R, soft_wall_set; # return \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1bba6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward matrix R dimensions: (900, 4)\n",
      "Number of soft wall states: 0\n",
      "Soft wall states represent budget violations ‚â• 1 USD\n"
     ]
    }
   ],
   "source": [
    "println(\"Reward matrix R dimensions: $(size(R))\")\n",
    "println(\"Number of soft wall states: $(length(soft_wall_set))\")\n",
    "println(\"Soft wall states represent budget violations ‚â• 1 USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1164cc27-d821-469c-bb3d-0ada67b2ff0c",
   "metadata": {},
   "source": [
    "#### Transition $T(s, s^{\\prime},a)$\n",
    "Next, build the transition function $T(s,s^{\\prime},a)$. We'll encode this as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ [multidimensional array](https://docs.julialang.org/en/v1/manual/arrays/) and populate it using nested `for` loops. I've already done this, so back away slowly from your keyboard and move on to the next task!\n",
    "\n",
    "But if you're curious, here's how it works:\n",
    "* In the `outer` loop, we iterate over actions. For every $a\\in\\mathcal{A}$, we get the move associated with that action and store it in the `Œî::Tuple`.\n",
    "* In the `inner` loop, we iterate over states $s\\in\\mathcal{S}$. We compute a `new_position` resulting from implementing action $a$ and check if `new_position`$\\in\\mathcal{S}$. If `new_position` is in the world and `current_position` is _not_ an `absorbing state`, we set $s^{\\prime}\\leftarrow$`world.states[new_position]` and `T[s, s‚Ä≤,  a] = 1.0`.\n",
    "* However, if the `new_position` is outside of the grid (or we are jumping from an `absorbing` state), we set `T[s, s,  a] = 1.0`, i.e., the probability that we stay in `s` if we take action `a` is `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "923e659b-22c8-4a3e-ab86-077b91e489b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "T = let\n",
    "\n",
    "    # --- DO NOT CHANGE ME .... PUT YOUR HANDS DOWN, AND SLOWLY STEP AWAY FROM YOUR KEYBOARD --------------- #\n",
    "    \n",
    "    # initialize -\n",
    "    T = Array{Float64,3}(undef, nstates, nstates, nactions);\n",
    "    fill!(T, 0.0);\n",
    "\n",
    "    # main loop -\n",
    "    for a ‚àà ùíú\n",
    "        Œî = world.moves[a]; # for this action, get the coordinate move\n",
    "        for s ‚àà ùíÆ\n",
    "            current_position = world.coordinates[s]\n",
    "            new_position =  current_position .+ Œî\n",
    "            if (haskey(world.states, new_position) == true && \n",
    "                    in(current_position, absorbing_state_set) == false)\n",
    "                s‚Ä≤ = world.states[new_position];\n",
    "                T[s, s‚Ä≤,  a] = 1.0\n",
    "            else\n",
    "                T[s, s,  a] = 1.0\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    # ----------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "    T # return\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5feb6e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix T dimensions: (900, 900, 4)\n",
      "T encodes deterministic transitions between states\n"
     ]
    }
   ],
   "source": [
    "println(\"Transition matrix T dimensions: $(size(T))\")\n",
    "println(\"T encodes deterministic transitions between states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f5445",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4f378-f223-4c44-9f17-abd0e41b6404",
   "metadata": {},
   "source": [
    "## Task 3: Use value iteration to estimate the optimal value function $U^{\\star}(s)$\n",
    "In Task 3, we construct a Markov Decision Process (MDP) instance and solve the problem using value iteration. The solution of the value iteration procedure is then used to estimate the optimal policy $\\pi^{\\star}(s)$. Toward this task:\n",
    "\n",
    "Construct an instance of the `MyMDPProblemModel`, which encodes the data required to solve the MDP problem. Pass the states `ùíÆ`, the actions `ùíú`, the transition matrix `T`, the reward matrix `R`, and the discount factor `Œ≥` into [the `build(...)` method](src/Factory.jl). We store the MDP model in the `m::MyMDPProblemModel` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bee066c6-f228-4d4d-a9ad-079cb6995b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = build(MyMDPProblemModel, (ùíÆ = ùíÆ, ùíú = ùíú, T = T, R = R, Œ≥ = Œ≥)); # build an mdp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b741bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP model constructed with:\n",
      "  - State space size: 900\n",
      "  - Action space size: 4\n",
      "  - Discount factor Œ≥: 0.95\n"
     ]
    }
   ],
   "source": [
    "println(\"MDP model constructed with:\")\n",
    "println(\"  - State space size: $(length(m.ùíÆ))\")\n",
    "println(\"  - Action space size: $(length(m.ùíú))\")\n",
    "println(\"  - Discount factor Œ≥: $(m.Œ≥)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb505c65-f9a2-4e9a-874b-8a7b06f5aec5",
   "metadata": {},
   "source": [
    "Next, call [the `mysolve(...)` method](src/Compute.jl) by passing a `value_iteration_model` instance and our MDP model `m::MyMDPProblemModel` as arguments. [The `mysolve(...)` method](src/Compute.jl) iteratively computes the optimal value function $U^{\\star}(s)$ and returns it in an instance of [the `MyValueIterationSolution` type](src/Types.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e41d060-5c23-4e9f-9c11-af63fa3a33f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `ErrorError` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `ErrorError` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] mysolve(model::MyValueIterationModel, problem::MyMDPProblemModel; œµ::Float64)\n",
      "   @ Main c:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646\\src\\Compute.jl:108\n",
      " [2] top-level scope\n",
      "   @ c:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X45sZmlsZQ==.jl:11"
     ]
    }
   ],
   "source": [
    "solution = let\n",
    "\n",
    "    # initialize - \n",
    "    solution = nothing;\n",
    "    k_max = 2500; # maximum number of iterations\n",
    "    œµ = 1e-8; # convergence tolerance\n",
    "    value_iteration_model = MyValueIterationModel(k_max); # takes k_max as argument\n",
    "\n",
    "    #COMPLETED: Call the mysolve(...) function to solve the MDP\n",
    "    #throw(ErrorException(\"Ooooops! You need to call the mysolve(...) function here!\"));\n",
    "    solution = mysolve(value_iteration_model, m, œµ=œµ);\n",
    "\n",
    "    # solution -\n",
    "    solution\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5d41c-b7c2-4ed4-bf8d-ed40a9eebdd7",
   "metadata": {},
   "source": [
    "Now, we extract the action-value function $Q(s, a)$ from the optimal value function $U^{\\star}(s)$. First, we estimate $Q(s,a)$ as:\n",
    "\n",
    "> __Policy__: To extract the policy, we need to compute the __state-action value function__ $Q(s,a)$ for each state and action. To estimate the policy, for each state $s \\in \\mathcal{S}$, compute:\n",
    "> $$\n",
    "\\begin{align*}\n",
    "\\pi^{*}(s) &\\gets \\arg\\max_{a\\in\\mathcal{A}_s}\\left(\\underbrace{R(s,a) + \\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T\\left(s^{\\prime}\\,|\\,s,a\\right)\\cdot{U^{*}}(s^{\\prime})}_{Q(s,a)\\text{ = brain of agent}}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "> We can do this using [the `QM(...)` function](src/Compute.jl), which takes `m::MyMDPProblemModel` and the `solution::MyValueIterationSolution`. \n",
    "\n",
    "Save the optimal $Q(s,a)$ in the `my_Q::Array{Float64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c091d3c-e3b9-4bd3-8195-8220d58fecce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `QM` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `QM` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X50sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "my_Q = QM(m, solution.U) # this computes the state-action value function Q(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d8d92-5880-49d6-910d-68d99da2a1fb",
   "metadata": {},
   "source": [
    "Finally, compute the optimal navigation policy $\\pi^{\\star}(s)$ from $Q(s,a)$ using [the `mypolicy(...)` function](src/Compute.jl). Save this in the `my_œÄ::Array{Int64,1}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6468d7ad-fac1-422b-b7f6-4cf7619a0544",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `mypolicy` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `mypolicy` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X52sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "my_œÄ = mypolicy(my_Q) # this tells us what we should do at each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b871ac79",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `my_œÄ` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `my_œÄ` not defined in `Main`\n",
      "Suggestion: add an appropriate import or assignment. This global was declared but not assigned.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X53sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "println(\"Optimal policy œÄ computed for all $(length(my_œÄ)) states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aaf91e-e201-43c2-8b84-d37a53df9c57",
   "metadata": {},
   "source": [
    "### Visualize the optimal policy\n",
    "The code block below shows how we visualize the decision-maker's path through the apples versus oranges space to arrive at the optimal solution. This code to visualize the optimal policy $\\pi^{\\star}(s)$ was modified from `L11d`.\n",
    "\n",
    "Specify an initial starting tuple `(apples, oranges)` in the `initial_site` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "338c7620-056d-4be0-bd96-e4434c004f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_site = (1, 30); # horizontal, vertical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86f775",
   "metadata": {},
   "source": [
    "This code block generates the plot showing the optimal navigation path from the starting position to the goal while avoiding hazards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4edb0539-29cb-4b00-a58e-2b4a370765bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `plot` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `plot` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X60sZmlsZQ==.jl:3"
     ]
    }
   ],
   "source": [
    "let\n",
    "    # draw the path -\n",
    "    p = plot();\n",
    "    hit_absorbing_state = false\n",
    "    s = world.states[initial_site];\n",
    "    visited_sites = Set{Tuple{Int,Int}}();\n",
    "    push!(visited_sites, initial_site);\n",
    "    \n",
    "    while (hit_absorbing_state == false)\n",
    "        current_position = world.coordinates[s]\n",
    "        a = my_œÄ[s];\n",
    "        Œî = world.moves[a];\n",
    "        new_position =  current_position .+ Œî\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:black, c=:blue)\n",
    "        plot!([current_position[1], new_position[1]],[current_position[2], new_position[2]], label=\"\", arrow=true, lw=1, c=:red)\n",
    "        \n",
    "        if (in(new_position, absorbing_state_set) == true || in(new_position, visited_sites) == true)\n",
    "            hit_absorbing_state = true;\n",
    "        else\n",
    "            s = world.states[new_position];\n",
    "            push!(visited_sites, new_position);\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # draw the grid -\n",
    "    for s ‚àà ùíÆ\n",
    "        current_position = world.coordinates[s]\n",
    "        a = my_œÄ[s];\n",
    "        Œî = world.moves[a];\n",
    "        new_position =  current_position .+ Œî\n",
    "        \n",
    "        if (haskey(rewards, current_position) == true && rewards[current_position] == my_objective_value)\n",
    "            scatter!([current_position[1]],[current_position[2]], label=\"Optimal: $(current_position)\", c=:green, ms=4, legend=:bottomleft)\n",
    "        elseif (in(current_position, soft_wall_set) == true)\n",
    "            scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:gray69, ms=4)\n",
    "        else\n",
    "            scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:gray50, c=:white)\n",
    "        end\n",
    "    end\n",
    "    xlabel!(\"Number of Apples\",fontsize=18)\n",
    "    ylabel!(\"Number of Oranges\",fontsize=18)\n",
    "    current()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e5f44",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "The plot above shows the optimal navigation path (red arrows with blue dots) from the starting position to the optimal consumption bundle (green dot). Notice the following key features:\n",
    "* The __green dot__ marks the optimal combination of apples and oranges computed in Problem 1.\n",
    "* The __gray region__ represents the soft-wall states where the budget constraint is violated by ‚â• 1 USD.\n",
    "* The __optimal path__ (red arrows) navigates through feasible states, avoiding the budget-violating region, and terminates at the goal state.\n",
    "* The policy guides the agent from any starting position toward the optimal consumption bundle while respecting the budget constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f10ad87",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb1f0a",
   "metadata": {},
   "source": [
    "### Exploration: Parameter Sensitivity Analysis\n",
    "Now that you have a working MDP solution, explore how different parameters affect the optimal policy and convergence behavior. Answer the following questions by modifying the appropriate parameters in the notebook above and re-running the relevant cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905db0d",
   "metadata": {},
   "source": [
    "#### Question 1: Effect of the Discount Factor\n",
    "What happens to the optimal policy when you decrease the discount factor $\\gamma$ from `0.95` to `0.50`? \n",
    "\n",
    "__Task__: Change $\\gamma$ to `0.50`, re-run the MDP construction and value iteration cells, and observe the resulting policy. Does the agent still navigate efficiently to the optimal consumption bundle? Why or why not?\n",
    "\n",
    "__Your Answer__: _(Write your observations and explanation here)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d84b1e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this flag to true once you have answered Question 1\n",
    "did_I_answer_question_1 = false;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838155e8",
   "metadata": {},
   "source": [
    "#### Question 2: Soft-Wall Tolerance Impact\n",
    "How does changing the soft-wall tolerance affect the size of the feasible region and the optimal policy?\n",
    "\n",
    "__Task__: Modify the budget violation threshold from `1.0 USD` to `5.0 USD` in the soft-wall setup (where we check `if (budget_violation ‚â• 1.0)`). Re-run the reward matrix construction and subsequent cells. How does the size of `soft_wall_set` change? Does the optimal path change significantly?\n",
    "\n",
    "__Your Answer__: _(Write your observations and explanation here)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9976913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this flag to true once you have answered Question 2\n",
    "did_I_answer_question_2 = false;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13a18",
   "metadata": {},
   "source": [
    "#### Question 3: Grid Resolution and Computational Cost\n",
    "What is the trade-off between grid resolution and computational performance?\n",
    "\n",
    "__Task__: Change the grid size from $30\\times 30$ to $50\\times 50$ by modifying `number_of_rows` and `number_of_cols`. Re-run all relevant cells from Task 1 onward. Compare the computation time for value iteration and observe whether the optimal consumption bundle changes (it should remain approximately the same since Problem 1 is independent of the grid). What is the computational cost of increasing the state space?\n",
    "\n",
    "__Hint__: You can measure execution time in Julia using the `@elapsed` macro, e.g., `time_taken = @elapsed mysolve(value_iteration_model, m)`.\n",
    "\n",
    "__Your Answer__: _(Write your observations and explanation here)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33a67532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this flag to true once you have answered Question 3\n",
    "did_I_answer_question_3 = false;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689d604",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135246ce",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We demonstrated that the apples and oranges resource allocation problem can be formulated and solved as a Markov Decision Process using value iteration.\n",
    "\n",
    "> Key takeaways:\n",
    "> * The optimal consumption bundle from nonlinear programming (Problem 1) served as the terminal state for the MDP, providing a clear goal for the value iteration algorithm to converge toward.\n",
    "> * Reward shaping with the Cobb-Douglas utility function $U(x)$ guided the MDP search by providing intermediate feedback, while soft-wall penalties enforced the budget constraint without hard boundaries.\n",
    "> * Value iteration successfully computed an optimal policy $\\pi^{\\star}(s)$ that navigates from any starting state to the optimal consumption bundle while respecting the budget constraint.\n",
    "\n",
    "This approach shows how classical optimization problems can be reframed as sequential decision-making problems, opening pathways to solve more complex resource allocation scenarios with uncertainty and dynamic constraints.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5944d4db",
   "metadata": {},
   "source": [
    "## Tests\n",
    "The code block below shows how we implemented the tests and what we are testing. In these tests, we check values in your notebook and give feedback on which items are correct, missing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42673044",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: `@testset` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nin expression starting at c:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y106sZmlsZQ==.jl:1",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: `@testset` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "in expression starting at c:\\Users\\Bhargav\\Documents\\GitHub\\ps5-cheme-5800-f25-BhargavCornell646\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y106sZmlsZQ==.jl:1\n",
      "\n",
      "Stacktrace:\n",
      " [1] eval(m::Module, e::Any)\n",
      "   @ Core .\\boot.jl:489\n",
      " [2] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "   @ Base .\\loading.jl:2842\n",
      " [3] (::VSCodeServer.var\"#notebook_runcell_request##0#notebook_runcell_request##1\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "   @ VSCodeServer c:\\Users\\Bhargav\\.vscode\\extensions\\julialang.language-julia-1.149.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:24\n",
      " [4] withpath(f::VSCodeServer.var\"#notebook_runcell_request##0#notebook_runcell_request##1\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "   @ VSCodeServer c:\\Users\\Bhargav\\.vscode\\extensions\\julialang.language-julia-1.149.2\\scripts\\packages\\VSCodeServer\\src\\repl.jl:276\n",
      " [5] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint, VSCodeServer.JSON.Serializations.StandardSerialization}, params::VSCodeServer.NotebookRunCellArguments, token::VSCodeServer.CancellationTokens.CancellationToken)\n",
      "   @ VSCodeServer c:\\Users\\Bhargav\\.vscode\\extensions\\julialang.language-julia-1.149.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:13\n",
      " [6] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint, VSCodeServer.JSON.Serializations.StandardSerialization}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::VSCodeServer.JSONRPC.Request)\n",
      "   @ VSCodeServer.JSONRPC c:\\Users\\Bhargav\\.vscode\\extensions\\julialang.language-julia-1.149.2\\scripts\\packages\\JSONRPC\\src\\typed.jl:68\n",
      " [7] serve_notebook(pipename::String, debugger_pipename::String, outputchannel_logger::Base.CoreLogging.SimpleLogger; error_handler::var\"#14#15\"{String})\n",
      "   @ VSCodeServer c:\\Users\\Bhargav\\.vscode\\extensions\\julialang.language-julia-1.149.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:147\n",
      " [8] top-level scope\n",
      "   @ c:\\Users\\Bhargav\\.vscode\\extensions\\julialang.language-julia-1.149.2\\scripts\\notebook\\notebook.jl:35"
     ]
    }
   ],
   "source": [
    "@testset verbose = true \"CHEME 5800 PS5 Test Suite\" begin\n",
    "\n",
    "    @testset \"Problem 1: Optimal Apples and Oranges (NLP)\" begin\n",
    "        \n",
    "        # Test that base_solution exists and has the correct structure\n",
    "        @test @isdefined(base_solution)\n",
    "        @test isa(base_solution, Dict)\n",
    "        @test haskey(base_solution, \"argmax\")\n",
    "        @test haskey(base_solution, \"objective_value\")\n",
    "        \n",
    "        # Test that optimal values are defined and are integers\n",
    "        @test @isdefined(optimal_apples)\n",
    "        @test @isdefined(optimal_oranges)\n",
    "        @test isa(optimal_apples, Int)\n",
    "        @test isa(optimal_oranges, Int)\n",
    "        \n",
    "        # Test that optimal values are non-negative\n",
    "        @test optimal_apples ‚â• 0\n",
    "        @test optimal_oranges ‚â• 0\n",
    "        \n",
    "        # Test that the solution respects reasonable bounds (not exact due to parameter flexibility)\n",
    "        @test optimal_apples < 100  # reasonable upper bound\n",
    "        @test optimal_oranges < 100 # reasonable upper bound\n",
    "        \n",
    "        # Test that Œ±, c, and B are defined with correct dimensions\n",
    "        @test @isdefined(Œ±)\n",
    "        @test @isdefined(c)\n",
    "        @test @isdefined(B)\n",
    "        @test length(Œ±) == 2\n",
    "        @test length(c) == 2\n",
    "        @test B > 0\n",
    "    end\n",
    "\n",
    "    @testset \"Task 1: Grid World Model Setup\" begin\n",
    "        \n",
    "        # Test that grid dimensions are defined\n",
    "        @test @isdefined(number_of_rows)\n",
    "        @test @isdefined(number_of_cols)\n",
    "        @test number_of_rows > 0\n",
    "        @test number_of_cols > 0\n",
    "        \n",
    "        # Test that state and action spaces are defined correctly\n",
    "        @test @isdefined(nstates)\n",
    "        @test @isdefined(nactions)\n",
    "        @test @isdefined(ùíÆ)\n",
    "        @test @isdefined(ùíú)\n",
    "        @test nstates == number_of_rows * number_of_cols\n",
    "        @test length(ùíÆ) == nstates\n",
    "        @test length(ùíú) == nactions\n",
    "        @test nactions == 4  # 4 cardinal directions\n",
    "        \n",
    "        # Test that discount factor is defined and in valid range\n",
    "        @test @isdefined(Œ≥)\n",
    "        @test 0 < Œ≥ ‚â§ 1\n",
    "        \n",
    "        # Test that rewards dictionary is defined and contains the optimal state\n",
    "        @test @isdefined(rewards)\n",
    "        @test isa(rewards, Dict)\n",
    "        @test haskey(rewards, (optimal_apples, optimal_oranges))\n",
    "        \n",
    "        # Test that absorbing state set is defined and contains the optimal state\n",
    "        @test @isdefined(absorbing_state_set)\n",
    "        @test isa(absorbing_state_set, Set)\n",
    "        @test in((optimal_apples, optimal_oranges), absorbing_state_set)\n",
    "        \n",
    "        # Test that world model is constructed correctly\n",
    "        @test @isdefined(world)\n",
    "        @test isa(world, MyRectangularGridWorldModel)\n",
    "        @test world.number_of_rows == number_of_rows\n",
    "        @test world.number_of_cols == number_of_cols\n",
    "        @test !isempty(world.coordinates)\n",
    "        @test !isempty(world.states)\n",
    "        @test !isempty(world.moves)\n",
    "    end\n",
    "\n",
    "    @testset \"Task 2: MDP Components (R and T matrices)\" begin\n",
    "        \n",
    "        # Test that R matrix is defined with correct dimensions\n",
    "        @test @isdefined(R)\n",
    "        @test isa(R, Array{Float64,2})\n",
    "        @test size(R) == (nstates, nactions)\n",
    "        \n",
    "        # Test that soft wall set is defined\n",
    "        @test @isdefined(soft_wall_set)\n",
    "        @test isa(soft_wall_set, Set)\n",
    "        \n",
    "        # Test that T matrix is defined with correct dimensions\n",
    "        @test @isdefined(T)\n",
    "        @test isa(T, Array{Float64,3})\n",
    "        @test size(T) == (nstates, nstates, nactions)\n",
    "        \n",
    "        # Test that T is a valid probability distribution (rows sum to 1 for each action)\n",
    "        for a in ùíú\n",
    "            for s in ùíÆ\n",
    "                @test sum(T[s, :, a]) ‚âà 1.0 atol=1e-10\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Test that all T values are non-negative\n",
    "        @test all(T .‚â• 0)\n",
    "        \n",
    "        # Test that MDP model is constructed\n",
    "        @test @isdefined(m)\n",
    "        @test isa(m, MyMDPProblemModel)\n",
    "        @test m.ùíÆ == ùíÆ\n",
    "        @test m.ùíú == ùíú\n",
    "        @test m.Œ≥ == Œ≥\n",
    "    end\n",
    "\n",
    "    @testset \"Task 3: Value Iteration and Policy Extraction\" begin\n",
    "        \n",
    "        # Test that solution is defined and has correct structure\n",
    "        @test @isdefined(solution)\n",
    "        @test hasfield(typeof(solution), :U)\n",
    "        \n",
    "        # Test that value function U has correct dimensions\n",
    "        @test length(solution.U) == nstates\n",
    "        \n",
    "        # Test that Q matrix is defined with correct dimensions\n",
    "        @test @isdefined(my_Q)\n",
    "        @test isa(my_Q, Array{Float64,2})\n",
    "        @test size(my_Q) == (nstates, nactions)\n",
    "        \n",
    "        # Test that policy is defined with correct dimensions\n",
    "        @test @isdefined(my_œÄ)\n",
    "        @test isa(my_œÄ, Array{Int64,1})\n",
    "        @test length(my_œÄ) == nstates\n",
    "        \n",
    "        # Test that all policy actions are valid (in action space)\n",
    "        @test all(a -> a ‚àà ùíú, my_œÄ)\n",
    "        \n",
    "        # Test that initial site is defined\n",
    "        @test @isdefined(initial_site)\n",
    "        @test isa(initial_site, Tuple{Int,Int})\n",
    "        @test initial_site[1] > 0 && initial_site[1] ‚â§ number_of_cols\n",
    "        @test initial_site[2] > 0 && initial_site[2] ‚â§ number_of_rows\n",
    "    end\n",
    "\n",
    "    @testset \"Helper Functions\" begin\n",
    "        \n",
    "        # Test the U function exists and works correctly\n",
    "        @test @isdefined(U)\n",
    "        \n",
    "        # Test U function with simple inputs\n",
    "        test_result = U((1, 1), [0.5, 0.5])\n",
    "        @test isa(test_result, Float64)\n",
    "        @test test_result ‚â• 0\n",
    "        \n",
    "        # Test U function with the optimal solution\n",
    "        optimal_utility = U((optimal_apples, optimal_oranges), Œ±)\n",
    "        @test isa(optimal_utility, Float64)\n",
    "        @test optimal_utility > 0\n",
    "    end\n",
    "\n",
    "    @testset \"Questions\" begin\n",
    "        @test did_I_answer_question_1 == true\n",
    "        @test did_I_answer_question_2 == true\n",
    "        @test did_I_answer_question_3 == true\n",
    "    end\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1084bf91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.1",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
